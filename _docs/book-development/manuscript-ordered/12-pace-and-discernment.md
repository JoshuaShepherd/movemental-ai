# Chapter 12: Pace and Discernment

I want to start this chapter by naming something you've probably felt: the pressure to rush. Everyone's using AI. Everyone's moving fast. If you're not adopting, you're falling behind. If you're not integrating, you're missing out.

I want to push back on that. Slowing down is responsible. Discernment takes time. And you can't responsibly decide when to use AI and when not to—when to abstain and when to accelerate—without first having space to experiment and think. This chapter is about that space, and about a practical framework for when to say yes, when to say caution, and when to say no.

## Slowing Down vs. Urgency vs. Rushing

Let me be direct: the cultural pressure in an AI age is toward urgency. FOMO. "Everyone's using AI." "You have to get ahead." "If you don't adopt now, you'll be left behind."

I want you to hear this: you don't have to rush. Slowing down is responsible. Here's why.

**First, discernment takes time.** We've said that experimentation is prerequisite to discernment. You can't responsibly answer "should I abstain?" or "where do I draw the line?" without having experimented. Experimentation requires space—low-stakes play, calibration, learning what AI does well and what it doesn't. That doesn't happen in a week. It might not happen in a month. Slowing down gives you that space.

**Second, the technology is still evolving.** We're three years in. The tools are changing. The best practices are still emerging. Rushing to adopt everything now might mean locking in patterns that don't serve you—or that don't serve your voice and your people. Slowing down lets you calibrate as the landscape shifts.

**Third, formation and integrity matter more than efficiency.** The book's stance is that formation and integrity matter more than efficiency. If you rush, you're optimizing for speed. If you slow down, you're optimizing for discernment, for voice, for boundaries. That's the optimization we're after.

So I want to give you permission: slow down. You don't have to adopt tomorrow. You don't have to integrate everywhere. You can stay in experimentation. You can take months to decide. That's responsible.

## AI as a Means of Slowing Down

Here's a twist: AI might actually help you slow down.

If you're spending hours on tasks that AI can help with—formatting, structure, variation, research—you have less time for reflection, for relationship, for formation. You're rushing not because you want to, but because the logistics are eating your time. AI can handle some of those logistics. It can draft. It can structure. It can adapt. And that can free you—not to do more of the same, but to do what only you can do: think, relate, form people, discern.

So AI as a means of slowing down means: use AI to handle the tasks that would otherwise force you to rush, so that you have time to slow down. Use it to create space for reflection, for relationship, for formation. Not to speed up indiscriminately—but to feel free to slow down.

I know that might sound counterintuitive. But it's worth sitting with. The goal isn't to do more faster. The goal is to do what matters, with integrity, at a pace that allows discernment. Sometimes AI serves that by taking tasks off your plate so you can breathe.

## When to Abstain and When to Accelerate: Green, Yellow, and Red Lights

So how do you decide when to use AI and when not to? I want to give you a practical framework—not a rigid rulebook, but a decision aid. Think of it as traffic lights: green, yellow, and red.

**Green lights: use cases where AI clearly amplifies without displacing what must stay human.** Examples: drafting and structure for content you'll revise and own; formatting and variation for content you've created; research and organization for thinking you're doing; administrative or logistical tasks that don't touch formation, relationship, or voice. In these cases, AI is handling what it does well—structure, expansion, variation, logistics—and you're providing what only you can provide—insight, voice, credibility, relationship. Green doesn't mean "use AI for everything." It means "this use case fits the amplification frame; proceed with transparency and boundaries."

**Yellow lights: use cases that require caution, context, and boundaries.** Examples: content that touches formation—teaching, discipleship materials, pastoral communication; voice-sensitive material—anything that carries your distinctive voice or theology; content that could be mistaken for fully human-generated without disclosure. In these cases, AI might help, but you need clear boundaries: more human revision, more oversight, explicit transparency, and a lower ratio of AI-to-human contribution. Yellow means "proceed with caution; hold the tension; maintain control."

**Red lights: use cases to refuse.** Examples: fully automated content publishing with no human review; AI-generated content presented as fully your own without disclosure; voice replacement—using AI to speak or write as if it were you in contexts where relationship or accountability matter; theological or formation content without human verification; formation work without human presence—discipleship, pastoral care, mentoring; deceptive practices—anything that misrepresents who or what produced the content. Red means "don't do this. The cost to credibility, formation, or integrity is too high."

This framework is a decision aid, not a law. Your context will have its own greens, yellows, and reds. The point is to have a way to think about it—and to slow down enough to use it.

## A Word of Encouragement

I know this chapter has been about pace and discernment and traffic lights. And that might feel like a lot to hold. You might be thinking, "How do I know which light I'm at?"

Here's what I want you to know: you don't have to have it all figured out. You have to experiment (green-light some low-stakes tries). You have to slow down enough to ask the question (yellow-light the pressure to rush). And you have to refuse the obvious reds (deception, replacement of relationship, formation without presence). The rest is discernment—and that takes time.

So take a breath. Slow down. Use the lights as a map, not a prison. And when you're not sure, err on the side of caution. That's responsible.

---

**Reflection Questions:**

1. Where have you felt pressure to rush with AI? What would it look like to slow down?

2. How might AI help you slow down—by handling tasks that currently force you to rush?

3. Think of one use case in your context. Is it green, yellow, or red? What would need to be true for it to be green?
