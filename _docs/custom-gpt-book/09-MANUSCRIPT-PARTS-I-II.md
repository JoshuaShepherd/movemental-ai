# MANUSCRIPT — Parts I & II: The Crisis, Reframe, and Who Is Speaking

---

# Preface: The Story

I want to tell you a story. Not because it's remarkable, but because it's real. Not because I figured it out, but because I didn't stop when I realized I hadn't.

This book is not primarily about AI theory. It's about what happened when I tried to build something real with AI, without knowing how, and refused to stop when it became overwhelming.

The project is called Movemental. It began in early September. And I want to tell you exactly where I was when I started, because I think it matters.

## Where I Started

I did not know anything.

That's not a humble admission—it's a diagnostic reality. This is not a story about a superhuman programmer. In fact, I did not know how to build a database. I did not know backend architecture.

As a full-stack developer, I had built one or two React apps—maybe three. One of them I launched on the iOS App Store and Android. That process went very quickly. About four weeks. No—actually, a week from start to finish.

That was my experience. And I falsely extrapolated that experience to a vastly more complex system. I thought: if I can build a front-end app in a week, I can build this. I didn't understand the scope of what I was attempting.

So the question is, if you didn't know anything, why did you believe you could do this?

I believed I could do this because I extrapolated from what I was experiencing, what I knew about myself, and my capacity to learn—to know how to solve problems I didn't understand or didn't know how to solve that day. I paired that with what I knew about AI as the technology existed at that moment, and what it could help me figure out.

And then in some way that belief grew during the project because I ended up using a technology I didn't have when I started.

But when I started, what I had was the knowledge that I didn't know how to do this—and that I had never done it before. I was operating with a level of ignorance that I didn't recognize as ignorance.

## What It Became

The scope evolved. I didn't plan this. It just happened.

It started as a tool. Then it became a machine. Then it became a system. Then it became a platform. Then it became a multi-tenant vision connecting real people—Alan Hirsch, Brad Briscoe, and others.

What began as an app to share Alan's work evolved into: an app that showcased AI, then one that demonstrated it, then one that prophetically modeled it. Which evolved into the realization that I already wanted to do this for Brad. Why not connect them? Why not use the same platform in a multi-tenant way?

It was supposed to be for Alan Hirsch. Along the way, we realized we could link Alan Hirsch and Brad Briscoe together.

I was learning daily. Adapting daily. Redesigning the system while building it. Adjusting not only code, but business vision and partnerships in real time. Learning everything I was doing every day—while adapting that learning to an evolving business vision. And doing that while also learning how to work agentically with AI.

There was no stable ground. Every day brought new understanding, new challenges, new decisions. I was building and learning and changing all at once.

If front-end apps take a short amount of time, and the hard work is the system underneath, then building the first app has little to do with the time it takes to build the second. There are complexities there, but the vision was always about multiplication. That vision changed the timeline because it made everything more adaptive.

## The Crisis Point

Around late October, I hit a point of internal crisis.

I had the internal conviction that I might have succumbed to what I understood as AI psychosis—the delusion that I was close to finishing something that would only ever be 80% complete and would fail.

I was afraid the project was an AI-induced delusion. I was afraid I was 80% done forever. I was afraid I had built something unfinishable.

I had been working nights and weekends, alongside a full-time job. I had been building other systems at the same time—six or seven applications, actually. CRM, front ends, moving fast because the front end was so fast. I was stretched thin, and I was starting to wonder if I was building something that couldn't be built.

I didn't know if I could finish. I didn't know if it was worth finishing. I didn't know if I was deluding myself about what was possible.

Essentially, I now know I was operating under a delusion—if the delusion was that I knew how to do this on my current trajectory. There was also a mistake—maybe not a delusion—of not knowing how much time it would take.

## The Friend and the Video

I reached out to the one and only friend I have who is a software developer.

Not to ask for help—but to say, "Look what I'm building."

Probably also to pay lip service to, "It's a little hard," but I don't think I did. I think I just thought, "I'm building this, and I'll get there."

He didn't say much. We talked. He shared a video. It seemed random—just supportive. "Good job." And then, "Here are some cool videos about building with AI."

But I remember watching that video and realizing I was beginning to be concerned that no matter how many times I tried the approach I was taking, it wasn't working.

I was trying that approach across six or seven applications—CRM, front ends—moving fast, because the front end was so fast. And it wasn't working. The approach wasn't working.

## The Turning Point

The turning point didn't come through confidence. It came through rethinking everything.

Over maybe a week—dated around 10/17 or 10/27, about a month and a half into the project—I began working with AI to design a system that would solve the technological problems in a way AI could actually help me.

That led to the creation of what I now call the type-safety chain. I realized I needed to design the system so that AI could actually help. Not just use AI to write code, but design the system in a way that made AI assistance possible, reliable, trustworthy.

I wasn't just building. I was learning how to build in a way that made AI useful, not just present. I was learning how to structure problems so that AI could help solve them.

This wasn't a breakthrough moment. It was a reorientation. A shift in how I was thinking about the problem, not a solution to the problem.

This is important because the project, when conceived not only as a technological project but as a business vision and partnership, has to tell a story of AI's power and potential—for good and bad—alongside a human story. A group of people working it out together.

## The Work

I kept working. Nights and weekends. Alongside a full-time job. Building other systems. Learning daily. Adapting daily. Changing daily.

The work was shared relationally, not privately. With Alan. With Brad. With others. I wasn't doing this alone, and I wasn't doing it in isolation. I shared everything with them during that time, as I had since day one.

I was wrong multiple times. I did not "figure it out." What mattered was refusing to stop grappling, and being willing to change in response to new information, in relationship.

## What I Realize Now

I realize now that I was wrong multiple times. I realize that I did not "figure it out." I realize that what mattered was refusing to stop grappling, and being willing to change in response to new information, in relationship.

I built this thinking I knew something.

I was wrong.

What I did next is changing my life again. I refused to stop experimenting—even when overwhelmed and confused. And I was willing to reconsider everything in light of new information shared in human relationship.

I don't have this figured out. I'm still learning. I'm still wrong about things. I'm still discovering what I don't know.

But I stayed. And I'm still staying. And that's the story I want to tell you.

## Why This Story Matters

If you don't understand the scope of what happened here, you won't understand how AI is changing the world. Any of it.

This isn't a story about someone who knew what they were doing. This is a story about someone who didn't know, and stayed anyway. This is a story about building something real with AI, without knowing how, and refusing to stop when it became overwhelming.

So if we translate the work done into a pre-AI frame and ask, "What happened?"—the truth of what happened is crucial. Because if you don't understand the scope of the work done, then you don't understand how AI is changing the world. Any of it.

If this person didn't know—and stayed—maybe you can stay too.

To alter Robert Frost:

I took the road less traveled.
I did it with friends.
And that has made all the difference.

---

*This story continues throughout the book. The grappling isn't finished. I'm still learning. And I invite you to walk with me.*

---

# Chapter 1: The Credibility Crisis

I want to start by acknowledging something that might feel uncomfortable: if you're reading this, you've probably already felt it. That moment when you read something online and wondered, "Is this real? Did a person actually write this, or was this generated?"

You're not alone in that uncertainty. Studies show that 68% of internet users now struggle to tell the difference between human-created and AI-generated content. That's not a small number. That's most of us.

Here's what I want you to know: your uncertainty is valid. The signals we used to rely on—the volume of someone's work, the polish of their presentation, their consistent online presence—these don't mean what they used to mean. AI can replicate all of them. And that creates a real problem for leaders like you who have something genuine to offer.

If you've spent years developing expertise, if you've invested in theological depth, if you've learned through real experience—you deserve to be heard. But right now, the mechanisms that used to help people find you are breaking down. The good news? There's a way forward. But first, we need to understand what we're up against.

## The Flood We're Swimming In

Let me give you some numbers that might surprise you. As of 2025, somewhere between 40% and 60% of the content you encounter online involves AI assistance or generation. That's not a prediction about the future. That's where we are right now.

Think about what that means. When you search for information about church planting, discipleship, missional theology, or any topic you care about—roughly half of what you find was created by machines, not people. Some of it is helpful. Some of it is accurate. But all of it raises a question: How do you know what's real?

I know this might feel overwhelming. You might be thinking, "I just want to share what I've learned. I just want to help people. Why does this have to be so complicated?"

I understand that feeling. But here's what I want you to see: this isn't just about technology. This is about trust. This is about how people know who to listen to, who to learn from, who to follow. And right now, those mechanisms are breaking down.

## When Volume Doesn't Mean Anything

There was a time—not that long ago, really—when the amount of content someone produced told you something about their expertise. If someone had written dozens of articles, published multiple books, maintained a blog for years, you could reasonably assume they knew what they were talking about. Volume suggested depth. Consistency suggested commitment.

But here's what's changed: AI can now produce that same volume in a fraction of the time. A single person with access to AI tools can generate more content in a week than most writers produce in a year. Entire websites can be populated overnight with thousands of articles, all perfectly formatted, all sounding authoritative.

I want you to understand what this means for you. If you're a pastor who's been writing for ten years, who's carefully developed your thoughts, who's learned through trial and error—your body of work might look small compared to someone who just started using AI last month. That's not fair. But it's where we are.

The old assumption—more content equals more expertise—doesn't hold anymore. Volume is no longer a reliable signal of credibility. And that's a problem, because volume was one of the ways people found you. It was one of the ways they decided you were worth listening to.

## When Polish Doesn't Signal Professionalism

I remember when I first started writing online. My early posts were rough. They had typos. The formatting was inconsistent. The ideas weren't fully developed. But over time, I got better. The polish of my writing improved because I was learning, growing, developing as a writer.

That progression—from rough to polished—used to tell a story. It showed development. It showed investment. It showed that someone cared enough to keep working at it.

But here's what's happening now: AI can make everything look polished from day one. A first-time writer using AI tools can produce content that looks as professional as someone who's been writing for decades. The formatting is perfect. The grammar is flawless. The structure is clear.

And that's wonderful, in some ways. It means more people can share their ideas. It means barriers to entry are lower. But it also means that polish—the thing that used to signal professionalism and experience—doesn't mean what it used to mean.

If you've spent years learning to write well, if you've invested in developing your voice, if you've worked hard to communicate clearly—your polished writing might look the same as someone who just asked AI to write something for them. That's not a reflection on your work. It's a reflection on how the landscape has changed.

## When Presence Doesn't Show Commitment

One of the things I've always appreciated about the leaders I respect is their consistency. They show up. They engage. They're present in the conversation, not just dropping in occasionally. That presence tells you something: this person is committed. This person cares. This person is in it for the long haul.

But here's what AI can do now: it can maintain that same presence indefinitely. An AI system can post consistently, respond to comments, engage with other content—all without a human being actively involved. It can create the appearance of commitment without the actual commitment.

I know this might sound cynical. I don't mean it that way. I'm just trying to help you understand what we're dealing with. The signals that used to help us identify committed, thoughtful leaders—they're not working the same way anymore.

If you're someone who has been faithfully showing up, who has been consistently engaging, who has been committed to the work over the long term—your presence might look the same as an AI system that's been running for a few weeks. That's not fair to you. But it's the reality we're navigating.

## The Trust Collapse

All of this—the flood of AI-generated content, the breakdown of credibility signals, the difficulty distinguishing real from artificial—it's creating what researchers call a "trust collapse." People are becoming skeptical of everything online, not because they're cynical, but because they can't tell what's real anymore.

Think about how that feels. You read something that sounds helpful, but you're not sure if you can trust it. You find someone who seems knowledgeable, but you wonder if they're actually an expert or just good at using AI. You want to learn, to grow, to find reliable sources—but the mechanisms for determining reliability are breaking down.

This trust collapse affects everyone, but it affects movement leaders in particular ways. Here's why:

**First, your expertise is hard-won.** You didn't develop your understanding of missional theology, church planting, discipleship, or whatever your area of focus is—you didn't develop that overnight. You've spent years studying, practicing, failing, learning, refining. That expertise is real. It's valuable. It deserves to be trusted. But in an environment where AI can generate plausible-sounding content on any topic, your real expertise becomes harder to distinguish from AI-generated fluency.

**Second, your credibility is relational.** The leaders I know who have real impact—they're not just sharing information. They're building relationships. They're creating trust through consistency, through vulnerability, through showing up over time. But when AI can replicate the appearance of consistency and presence, the relational foundation of credibility becomes harder to establish.

**Third, your voice is distinctive.** You have a way of thinking, a way of communicating, a way of seeing the world that's uniquely yours. That distinctiveness is part of what makes your work valuable. But when AI can mimic styles and voices, your distinctive voice becomes harder to recognize and trust.

## Why This Matters for Movement Leaders

I want to pause here and speak directly to why this matters for you, specifically, as a movement leader.

Movement leaders are different from other kinds of leaders. You're not just sharing information. You're not just building a platform. You're catalyzing transformation. You're calling people into something bigger than themselves. You're creating movements that multiply, that spread, that change communities and cultures.

That work requires trust. It requires credibility. It requires people to know that you're not just saying things that sound good—you're saying things that are true, that are tested, that come from real experience and deep reflection.

But here's the problem: in an AI-saturated world, that trust is harder to establish. The mechanisms that used to help people recognize trustworthy leaders—they're breaking down. And that makes your work harder.

I know this might feel discouraging. You might be thinking, "I've spent years developing expertise. I've invested in learning. I've worked hard to communicate clearly. And now I have to compete with AI-generated content?"

I want you to hear this: you're not competing with AI. You're navigating a new landscape where the rules have changed. And the good news is, there's a way forward. But it requires understanding what's happening, and then responding in ways that honor both the opportunities and the challenges of this moment.

## AI as Both Problem and Solution

Before we go further, I need to name something that might feel contradictory: AI is both the problem and the solution.

AI creates the credibility crisis we've been talking about. It can generate infinite content on any topic. It can produce articles, books, social media posts, sermons—you name it—quickly, consistently, at scale. When anyone can generate unlimited amounts of content, the internet becomes flooded. Some of it's helpful. Some of it's accurate. But much of it is noise—content created not because someone has something to say, but because they can. AI-generated content often looks credible. It's well-formatted. It sounds authoritative. But it doesn't have the foundation—the years of study, the real experience—that makes your content valuable.

But here's the thing: AI is also providing tools that can help us navigate this crisis. AI can amplify your voice. It can help you communicate more clearly, more consistently, more effectively. It can help you reach more people, engage more deeply, multiply your impact. But here's the key: it amplifies your voice. It doesn't replace it. If you're a pastor who preaches on Sunday, AI can help you turn that sermon into a blog post, a social media series, a newsletter article. It can help you adapt your message for different audiences. But the message is still yours. The voice is still yours. The expertise is still yours.

So we're in this tension: we're being asked to use the tool that created the problem. On one hand, ignoring AI isn't realistic. It's here. It's changing how content is created and consumed. If you try to ignore it, you'll find yourself increasingly invisible. On the other hand, uncritical adoption is dangerous. If you just start using AI without thinking through the implications, you risk losing what makes your voice distinctive. You risk contributing to the very problem you're trying to solve.

I know this tension might feel uncomfortable. You might be thinking, "Can't we just avoid this?" I understand that desire. But the landscape has already changed. Whether you use AI or not, you're operating in a world where AI exists. The question isn't whether to engage with AI. The question is how to engage with it well.

Ignoring AI isn't an option—because AI is already shaping the landscape, the mechanisms for discovery are changing, your audience is using AI, and efficiency matters for the things only you can do. But uncritical adoption is dangerous—because it can erode your voice, undermine your credibility, replace what only you can do, and contribute to the problem. Navigating this well requires both engagement and boundaries. We'll get to what that looks like.

## What We're Really Talking About

This isn't about whether AI is good or bad. This isn't about whether you should use AI tools or avoid them. This is about understanding how credibility works in a world where AI exists.

The question isn't: Should AI exist?  
The question is: How do we maintain credibility when AI can replicate so many of the signals we used to rely on?

The question isn't: Should we use AI?  
The question is: How do we use AI in ways that preserve and amplify our authentic voice, rather than replacing it?

And here's what I want you to know: there is a solution. It's not going back to the old ways—those mechanisms are broken. It's not ignoring AI—that's not realistic. It's building something new: credibility through networks, through relationships, through human verification that AI can't easily replicate. And it's reframing the challenge itself—not as a technology problem, but as a people problem. We'll get to that next.

## A Word of Encouragement

I know this chapter has been heavy. We've talked about problems and breakdowns and trust collapse and the tension of using the tool that created the problem. And I want to make sure you hear this: there's hope here. There's a way forward.

Your expertise is still valuable. Your voice is still distinctive. Your commitment is still meaningful. What's changed is how that value gets recognized. What's changed is how people find you. What's changed is how credibility gets established. And that's actually an opportunity, if we can figure out how to navigate it well.

In the next chapter, we're going to do something important: we're going to reframe what we're facing. Not as a technological challenge—but as an anthropological one. A people problem. And therefore an adaptive leadership task. That reframe changes everything about who can lead here, and how.

So take a breath. Process what we've talked about. And when you're ready, we'll move forward together.

---

**Reflection Questions:**

1. When was the last time you questioned whether something you read online was AI-generated? How did that feel?

2. Which of the broken credibility signals (volume, polish, presence) has affected you most personally?

3. How do you feel about the tension of using the tool that created the problem? What concerns does it raise?

---

# Chapter 2: AI Is Not a Technological Challenge

I want to start this chapter with a reframe that might change how you see everything that follows.

AI is not, first and foremost, a technological problem. It's an anthropological one.

This distinction matters because it changes how we understand what we're facing. If AI were primarily a technological problem, we could solve it with better algorithms, more data, faster processors, smarter systems. But AI is fundamentally a human problem—a problem that has to do with questions about human flourishing, relationship, ethics, vocation, change, and what it means to be human in a world where machines can mimic, augment, and potentially replace human capacities.

And here's what I want you to know: the way we actually interact with AI doesn't require technological understanding. We don't "interface" with it in the way we do with code or infrastructure. We talk to it. We write to it. We read what it writes back. It's mostly written or verbal communication. Optimizing and understanding AI does not require you to be technical. Non-technical leaders can lead here. That's good news for movement leaders who create or steward content—you're already experts in communication. The challenge is a people challenge. And therefore it's an adaptive leadership task.

## How We Actually Interact With AI

Let me be clear about what I mean. When I say we interact with AI through written or verbal communication, I mean that's the primary way most of us encounter it. We type a question. We get a response. We refine, we ask again, we iterate. We might speak to it. We might paste in a document and ask for feedback. We're not writing code to "interface" with the system. We're in conversation.

That means the skills that matter are the skills you already have: clarity, specificity, the ability to refine and iterate, the ability to judge whether a response is helpful or off. You don't need to understand how the model works under the hood. You need to understand how to communicate well, how to hold the tension between trust and verification, and how to lead people through change. Those are anthropological skills. Leadership skills. Formation skills.

So when someone tells you that you need to become technical to navigate AI, I want you to push back. You need to become wise about communication, about credibility, about what AI displaces and what it amplifies. You don't need to become an engineer.

## The Anthropological Problem

Why does it help to call this an anthropological problem? Because the research reveals that people aren't worried about the technology itself—they're worried about what it means for human flourishing. About half of people believe AI will worsen their ability to form meaningful relationships. About half believe it will worsen creative thinking. About two-thirds believe it will eliminate more jobs than it creates. These aren't technical concerns. They're anthropological ones. They're questions about what it means to be human, how humans flourish, and how we organize human life in ways that serve human good.

The anthropological problem of AI touches every dimension of human life:

**Human flourishing.** What does it mean to flourish in a world where AI can do many things humans do? If work is essential to human identity and dignity, what happens when AI can do that work? If creativity is central to human expression, what happens when AI can generate creative content? If relationship is fundamental to human flourishing, what happens when people form relationships with AI systems?

**Relationship.** The research shows growing concern about AI's impact on human connection. But it also shows people forming relationships with AI—companions, confidants, even romantic partners. This raises profound questions: What is authentic relationship? Can relationship with AI support or undermine human relationships? How do we navigate the boundary between tool and companion?

**Ethics.** AI forces us to ask fundamental ethical questions: What is the good? Who benefits from AI, and who bears its costs? How do we ensure AI serves human dignity rather than undermining it?

**Vocation.** If work is essential to human identity, what happens when AI can do that work? What is the purpose of human work in an AI age? How do we understand calling and vocation when machines can perform many of the tasks that have defined human work?

**Change.** AI represents change at a scale and speed that's unprecedented. How do humans adapt to such rapid change? What does it mean to lead through change that's happening faster than we can fully understand?

These are not technological questions. They're anthropological ones. And because they're anthropological, they're also adaptive.

## Therefore It's an Adaptive Leadership Task

Adaptive problems are different from technical problems. Technical problems have known solutions that can be applied by experts. Adaptive problems require learning, experimentation, and change—not just in systems, but in people, relationships, and communities.

Ronald Heifetz distinguishes between technical and adaptive challenges. Technical challenges can be solved with existing knowledge and expertise. Adaptive challenges require people to change their values, beliefs, behaviors, and ways of working. AI is fundamentally an adaptive challenge because it requires us to change how we understand ourselves, our work, our relationships, and our communities.

This means that the call to lead with AI is the call to adaptive leadership. Adaptive leadership isn't about having all the answers—it's about creating the conditions for learning, experimentation, and change. It's about helping people navigate uncertainty, ambiguity, and loss. It's about maintaining stability while enabling transformation. It's about holding space for the anxiety, grief, and resistance that come with adaptive change.

Adaptive leadership with AI requires several things:

**Acknowledging loss.** Adaptive change involves loss. When AI changes how we work, relate, and organize, we lose familiar ways of being. Adaptive leaders acknowledge this loss rather than minimizing it. They create space for grief, anxiety, and resistance rather than trying to eliminate it.

**Creating learning environments.** Adaptive challenges require learning, not just training. Adaptive leaders create environments where people can experiment, fail, learn, and adapt. They model learning themselves, showing that they don't have all the answers.

**Holding tension.** Adaptive change involves tension—between old and new, between stability and change, between individual and collective needs. Adaptive leaders hold this tension rather than resolving it prematurely. They help people navigate ambiguity without providing false certainty.

**Protecting voices.** In adaptive change, some voices are marginalized or silenced. Adaptive leaders protect voices that need to be heard, especially voices that challenge the status quo or raise uncomfortable questions.

**Regulating distress.** Adaptive change creates distress. Adaptive leaders help people manage this distress—not by eliminating it, but by keeping it at a productive level where learning can happen.

But here's the crucial point: adaptive leadership cannot be done alone. Adaptive challenges require community. They require people working together, learning together, experimenting together, and supporting each other through change. No leader can navigate adaptive change in isolation.

This is where scenius becomes essential. Scenius—the collaborative genius that emerges from networks of relationships—isn't just a credibility solution. It's a community solution to an adaptive challenge. When movement leaders are connected through scenius, they're not just verifying each other's credibility—they're creating the relationships that enable adaptive learning. The network relationships that form scenius are the same relationships that enable collaborative learning, mutual support, and collective adaptation. We'll get to scenius in more detail later. For now, I want you to hold this: the anthropological problem of AI requires adaptive leadership, which requires community. You don't need to be a technologist. You need to be a leader who can create the conditions for learning and change, in relationship with others.

## What This Means for You

So what does this reframe mean for you, as a movement leader?

First, you're not disqualified because you're not technical. The challenge is a people challenge. You're qualified to lead here precisely because you already lead people—you form them, you communicate with them, you build trust with them.

Second, you're not being asked to have all the answers. You're being asked to create the conditions for learning. To hold tension. To acknowledge loss. To regulate distress. To protect voices. Those are leadership skills you already have or can develop.

Third, you're not alone. Adaptive leadership requires community. The relationships you're building—with other leaders, with your network, with the people who vouch for you and you for them—those aren't optional add-ons. They're the infrastructure that makes adaptive change possible.

AI is not, first and foremost, a technological problem. It's an anthropological one. And that changes everything about how we lead, how we learn, and how we build the communities that will navigate this change together.

---

**Reflection Questions:**

1. Where have you assumed you needed to be "technical" to lead with AI? How does the anthropological reframe change that?

2. What losses have you or your people already experienced as AI has entered your context? How might acknowledging those losses change your leadership?

3. Who is in your community for adaptive learning? Who are you learning with?

---

# Chapter 3: Where We Are and Why Now

Where are we? This question matters because where we are determines what we need. And when it comes to AI, where we are is changing rapidly—so rapidly that writing a single book that speaks to everyone's place feels almost impossible.

Let me start with a fundamental reality: AI, as we're experiencing it now, is just over three years old. For everyone except AI scientists and science fiction writers, this is brand new territory. The rest of us never saw this coming. Period.

This matters because it means we're all learning in real time. There's no established playbook. There's no generation of elders who've navigated this before us. We're the first generation to face this particular challenge, and we're doing it while the technology is still evolving at breakneck speed.

I see my role as a guide. In many ways, I've been going down this trail my whole life—thinking about technology, communication, community, and how tools shape us. But in many ways, that trail wasn't possible until three short years ago. I did start on day one. I have not stopped. I've obsessively charted what's possible and terrifying about AI, and I've worked closely with Alan Hirsch and Brad Briscoe to build this.

But where is our audience? This question haunts me because the research reveals something important: we're not all in the same place. Not even close.

## The Landscape We're In

According to recent surveys, about 95% of adults have heard at least a little about AI, with 47% saying they've heard a lot—nearly double since 2022. But awareness doesn't mean understanding, and understanding doesn't mean comfort. About 62% of U.S. adults report interacting with AI at least several times a week, but that interaction ranges from casual use to deep integration. Among younger adults under 30, around one-third engage with AI several times a day. But even among regular users, there's significant variation in understanding, comfort, and concern.

The research reveals a landscape of different places:

**The Overwhelmed.** About 50% of U.S. adults say they're more concerned than excited about AI, up from 37% in 2021. Only about 10% say they're more excited than concerned. Roughly 57% rate the risks of AI for society as high or very high, while only 25% say the same about benefits. About 60% want more control over how AI is used in their lives, but only 13% feel they have a lot of control. These are people who feel the technology is moving too fast, who don't understand it, who feel powerless in the face of change.

**The Anxious.** About 49% believe AI will create significant job displacement. About 50% think AI will worsen people's ability to form meaningful relationships. About 53% believe AI will worsen creative thinking. About 67% believe AI will eliminate more jobs than it creates. These are people who see the risks clearly—job loss, erosion of human connection, loss of skills, economic disruption. They're not wrong to be concerned.

**The Disengaged.** While 95% have heard about AI, that still means 5% haven't. And even among those who have heard, many haven't engaged. They're not worried because they're not paying attention. They're not interested because it hasn't touched their lives directly yet. But that's changing rapidly.

**The Enthusiastic Early Adopters.** About one-third of adults under 30 engage with AI several times a day. These are people who've integrated AI into their daily workflows, who see the potential, who are experimenting and learning. But even among this group, there's significant variation in understanding and wisdom about how to use AI well.

**The Overconfident.** Recent research on the Dunning-Kruger effect and AI reveals something troubling: people using AI tend to overestimate their performance and understanding. AI assistance boosts actual performance but inflates self-confidence disproportionately. The classic pattern where low performers overestimate and high performers underestimate is being flattened—now, both groups overestimate when using AI. Counterintuitively, people with higher AI literacy tend to be less accurate about their performance, with more inflated self-assessments. This is the group that thinks they understand AI better than they do.

Where are we? We're all over the map. And this creates a fundamental challenge: how do we write one book that speaks to people who are overwhelmed, anxious, disengaged, enthusiastic, overconfident—all at the same time?

But here's the complicating factor: where people are is changing rapidly. Someone who's disengaged today might be overwhelmed tomorrow when AI touches their job. Someone who's enthusiastic today might become anxious tomorrow when they realize the implications. Someone who's overconfident today might become humble tomorrow when they encounter the limits of their understanding.

This rapid change makes me wonder about Dunning-Kruger more than anything. I wonder if we've even begun to learn. The research shows that AI use can lead to "cognitive offloading"—accepting AI's answers without questioning or deeply engaging. This cuts off feedback loops people rely on to calibrate how good they are. So performance might improve, but self-awareness of errors or limitations declines.

Have we even begun to learn? Or are we in the early stages of a massive overconfidence crisis? I don't know. But I suspect we're still in the early days of learning, and that the real education is just beginning.

## Why This Moment Is Different

I want to pause here and speak to something that might feel validating: movement leaders were right to be skeptical of metrics-driven content. You were right to resist the pressure to optimize for algorithms. You were right to prioritize depth over clicks, transformation over engagement, authenticity over reach.

In the old model, credibility was conferred by gatekeepers. If you wanted to be a credible voice, you needed to be published by a respected publisher, invited to speak at respected conferences, endorsed by respected institutions. When the digital revolution came—when blogs emerged, then social media, then content marketing, then SEO optimization—many movement leaders were skeptical. And that skepticism was often justified. The metrics didn't measure what mattered. The platforms rewarded extraction, not formation. The optimization pressure felt inauthentic. The gatekeepers still mattered. Movement leaders recognized that the old signals still worked, at least to some degree. And that the new signals—the metrics, the algorithms, the platform optimization—felt like a compromise.

But something has changed. The gatekeepers are losing power. AI has flooded the landscape. Your audience has changed how they discover content and leaders. The rules have changed. The old model—where gatekeepers conferred credibility, where you could build a following through books and conferences and endorsements—that model is breaking down. The new model—where credibility emerges through networks, through relationships, through visible engagement—requires different strategies. And here's what makes this moment different from the digital disruption of the 2000s and 2010s: AI has changed the cost of content creation. In the old digital model, creating content took time. AI has removed that limit. Now content can be generated at almost zero cost. Credibility signals are collapsing faster. And what's emerging as the alternative is network verification—credibility through relationships, through mutual vouching, through scenius. This is a fundamentally different mechanism than what we had before.

I'm not saying the old instincts were wrong. I'm saying the landscape has changed enough that the old instincts, applied in the same ways, might not be enough anymore. What's needed now is not abandoning your instincts, but adapting them. Holding onto what matters—depth, authenticity, transformation—while engaging with the new mechanisms for discovery and credibility. Finding ways to be visible without compromising what makes your voice valuable.

## Where We Start

Where are we? We're in different places, and those places are changing rapidly. But we're all in this together, learning in real time, trying to figure out how to navigate a technology that's reshaping everything—and doing it while the technology itself is still evolving.

The challenge is meeting people where they are while acknowledging that where they are is changing. The opportunity is creating content that can adapt, that can speak to different places, that can serve people whether they're overwhelmed or enthusiastic, anxious or overconfident, disengaged or deeply integrated.

This is why I see my role as a guide. Not because I have all the answers—I don't. Not because I've figured it all out—I haven't. But because I've been on this trail from day one, I've been charting what's possible and terrifying, and I've been working to build something that serves movement leaders well.

Where are we? We're here, together, learning. And that's where we start.

---

**Reflection Questions:**

1. Where do you find yourself on this landscape? Overwhelmed? Anxious? Enthusiastic? Somewhere else?

2. How have your instincts about digital platforms and metrics served you? What have they protected?

3. What would it look like to adapt your instincts to this new landscape, without compromising what matters?

---

# Chapter 4: Finding a Guide

I want to start this chapter by acknowledging something that might feel uncomfortable: I don't know if you need an AI expert. I'm not even sure what that means anymore.

This chapter exists because of something I've been wrestling with for the last three years. When ChatGPT was released publicly in November 2022, I started on day one. I have not stopped. I've obsessively charted what's possible and terrifying about AI. I've worked closely with Alan Hirsch and Brad Briscoe to build something that serves movement leaders. And in many ways, I've been going down this trail my whole life—thinking about technology, communication, community, and how tools shape us.

But here's what I've learned: even with all that intensity, even with all that focus, I'm still learning. I'm still wrong about things. I'm still discovering what I don't know. And that's not a failure—it's the reality of what we're facing.

This chapter exists because AI expertise is historically unprecedented. With the exception of AI scientists and science-fiction writers—neither of whom are generally available to churches or movemental organizations—no one has more than about three years of lived experience with modern AI. Therefore, every person claiming to be an "AI expert" is operating within a radically compressed, discontinuous timeline.

This is not a critique. It's a diagnostic reality. And it changes everything about how we think about expertise, guidance, and what we actually need.

## The Three-Year Reality

Let me be direct about the timeline, because I think we need to sit with this: AI, as we're experiencing it now, is just over three years old. For everyone except AI scientists and science fiction writers, this is brand new territory. The rest of us never saw this coming. Period.

This matters because it means we're all learning in real time. There's no established playbook. There's no generation of elders who've navigated this before us. We're the first generation to face this particular challenge, and we're doing it while the technology is still evolving at breakneck speed.

No one has decades of AI wisdom. Anyone claiming mastery is either exaggerating, redefining "expert," or unaware of what they don't know.

And I need to say this: I have not been immune to this. I've overestimated my understanding. I've been overconfident. I've thought I had things figured out, only to realize I was wrong. The Dunning-Kruger effect is real, and it affects all of us—including me.

## Who Are Today's "AI Experts," Really?

This question matters because if we're going to find guidance, we need to understand who's offering it. And I want to be clear: I'm not mocking anyone here. I'm trying to understand what we're actually dealing with.

When I look at who's presenting as AI experts today, I see a range of backgrounds: consultants who've pivoted to AI consulting, coaches who've added AI to their toolkit, executives who've led AI initiatives, leadership speakers who've integrated AI into their talks, technologists with narrow scopes—people who understand specific AI tools or applications, but may not understand the broader implications for human formation, community, or discipleship.

And here's the question I keep coming back to: Do these backgrounds prepare someone for adaptive human change, or merely for tactical optimization?

I don't have a definitive answer. But I think the question matters. Because if AI is discontinuous—if it represents a break from prior patterns rather than an extension of them—then the markers of credible guidance might need to change.

## If AI Is Discontinuous, What Kind of Expertise Actually Matters?

This is the heart of the chapter. If AI is discontinuous, then the markers of credible guidance change. And I want to share with you what I've been looking for—not as a definitive list, but as emerging criteria that I'm still refining.

**Evidence of intense grappling over the last three years.** Not just fluency, but wrestling. Not just using AI, but struggling with it. Not just adopting it, but questioning it. I'm looking for signs that someone has been in the tension, not just on one side of it.

**Evidence of building.** Systems, tools, workflows—not just commentary. I'm looking for people who've created something, who've tried to solve real problems, who've learned through doing, not just through thinking or talking.

**Evidence of rapid evolution.** Multiple paradigm shifts, rewrites, discarded approaches. I'm looking for people who've changed their minds, who've abandoned what they thought they knew, who've been willing to start over.

**Visible discomfort paired with clarity.** Confidence without certainty. I'm looking for people who are clear about what they know, but uncomfortable with claiming more than they know. People who can say "I don't know" without losing credibility.

**Resistance to totalizing answers.** Absence of "I've figured it out" energy. I'm looking for people who resist the temptation to have all the answers, who maintain curiosity, who stay in the questions.

**A maturity map.** Some coherent vision of human formation in light of AI. I'm looking for people who've thought about what it means to be human, to form people, to build community, in an AI age. Not just tactical optimization, but formation-level thinking.

**Personalized, specific vision.** Not generic frameworks. I'm looking for people who've thought deeply about specific contexts, specific challenges, specific communities. People who can speak to particular situations, not just universal principles.

**Textual intelligence.** Deep comfort thinking and reasoning in language. I'm looking for people who can think clearly, communicate precisely, reason carefully. People who understand that words matter, that language shapes thought, that how we talk about AI affects how we use it.

**Love.** Genuine care for people over opportunity. I'm looking for people who care more about serving others than about building their platform, who prioritize people's formation over their own advancement, who demonstrate love in how they show up.

Look for love.

## From Expert to Guide

I want to end this chapter by pivoting, not resolving. Because I don't think the question is whether you need an AI expert. I think the question is whether you need a guide.

**Expert = someone with answers.** An expert is someone who knows, who has figured it out, who can tell you what to do. An expert operates from certainty, from mastery, from having arrived.

**Guide = someone who accompanies amid uncertainty.** A guide is someone who walks with you, who learns with you, who helps you navigate uncertainty without pretending to have all the answers. A guide operates from experience, from wisdom, from being on the journey.

The difference is existential, not semantic. If AI is discontinuous, if we're all learning in real time, if no one has decades of wisdom, then what we need is not someone who claims to have arrived, but someone who's willing to walk with us as we figure it out together.

You may not need an AI expert. But you may need a guide. And the people who can guide you through this moment may not look like traditional experts. They may look like people who've been grappling, who've been building, who've been learning, who've been wrong, who've been willing to change their minds, who care more about your formation than their platform.

---

**Reflection Questions:**

1. What have you been looking for in AI guidance? Has this chapter changed what you're looking for?

2. Which of the criteria resonates most with you? Which challenges you?

3. What's the difference between needing an expert and needing a guide? What do you actually need right now?

