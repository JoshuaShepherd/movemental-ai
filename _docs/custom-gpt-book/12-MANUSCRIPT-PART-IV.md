# MANUSCRIPT — Part IV: Pace and Discernment (Chapters 12–13)

---

# Chapter 12: Pace and Discernment

I want to start this chapter by naming something you've probably felt: the pressure to rush. Everyone's using AI. Everyone's moving fast. If you're not adopting, you're falling behind. If you're not integrating, you're missing out.

I want to push back on that. Slowing down is responsible. Discernment takes time. And you can't responsibly decide when to use AI and when not to—when to abstain and when to accelerate—without first having space to experiment and think. This chapter is about that space, and about a practical framework for when to say yes, when to say caution, and when to say no.

## Slowing Down vs. Urgency vs. Rushing

Let me be direct: the cultural pressure in an AI age is toward urgency. FOMO. "Everyone's using AI." "You have to get ahead." "If you don't adopt now, you'll be left behind."

I want you to hear this: you don't have to rush. Slowing down is responsible. Here's why.

**First, discernment takes time.** We've said that experimentation is prerequisite to discernment. You can't responsibly answer "should I abstain?" or "where do I draw the line?" without having experimented. Experimentation requires space—low-stakes play, calibration, learning what AI does well and what it doesn't. That doesn't happen in a week. It might not happen in a month. Slowing down gives you that space.

**Second, the technology is still evolving.** We're three years in. The tools are changing. The best practices are still emerging. Rushing to adopt everything now might mean locking in patterns that don't serve you—or that don't serve your voice and your people. Slowing down lets you calibrate as the landscape shifts.

**Third, formation and integrity matter more than efficiency.** The book's stance is that formation and integrity matter more than efficiency. If you rush, you're optimizing for speed. If you slow down, you're optimizing for discernment, for voice, for boundaries. That's the optimization we're after.

So I want to give you permission: slow down. You don't have to adopt tomorrow. You don't have to integrate everywhere. You can stay in experimentation. You can take months to decide. That's responsible.

## AI as a Means of Slowing Down

Here's a twist: AI might actually help you slow down.

If you're spending hours on tasks that AI can help with—formatting, structure, variation, research—you have less time for reflection, for relationship, for formation. You're rushing not because you want to, but because the logistics are eating your time. AI can handle some of those logistics. It can draft. It can structure. It can adapt. And that can free you—not to do more of the same, but to do what only you can do: think, relate, form people, discern.

So AI as a means of slowing down means: use AI to handle the tasks that would otherwise force you to rush, so that you have time to slow down. Use it to create space for reflection, for relationship, for formation. Not to speed up indiscriminately—but to feel free to slow down.

I know that might sound counterintuitive. But it's worth sitting with. The goal isn't to do more faster. The goal is to do what matters, with integrity, at a pace that allows discernment. Sometimes AI serves that by taking tasks off your plate so you can breathe.

## When to Abstain and When to Accelerate: Green, Yellow, and Red Lights

So how do you decide when to use AI and when not to? I want to give you a practical framework—not a rigid rulebook, but a decision aid. Think of it as traffic lights: green, yellow, and red.

**Green lights: use cases where AI clearly amplifies without displacing what must stay human.** Examples: drafting and structure for content you'll revise and own; formatting and variation for content you've created; research and organization for thinking you're doing; administrative or logistical tasks that don't touch formation, relationship, or voice. In these cases, AI is handling what it does well—structure, expansion, variation, logistics—and you're providing what only you can provide—insight, voice, credibility, relationship. Green doesn't mean "use AI for everything." It means "this use case fits the amplification frame; proceed with transparency and boundaries."

**Yellow lights: use cases that require caution, context, and boundaries.** Examples: content that touches formation—teaching, discipleship materials, pastoral communication; voice-sensitive material—anything that carries your distinctive voice or theology; content that could be mistaken for fully human-generated without disclosure. In these cases, AI might help, but you need clear boundaries: more human revision, more oversight, explicit transparency, and a lower ratio of AI-to-human contribution. Yellow means "proceed with caution; hold the tension; maintain control."

**Red lights: use cases to refuse.** Examples: fully automated content publishing with no human review; AI-generated content presented as fully your own without disclosure; voice replacement—using AI to speak or write as if it were you in contexts where relationship or accountability matter; theological or formation content without human verification; formation work without human presence—discipleship, pastoral care, mentoring; deceptive practices—anything that misrepresents who or what produced the content. Red means "don't do this. The cost to credibility, formation, or integrity is too high."

This framework is a decision aid, not a law. Your context will have its own greens, yellows, and reds. The point is to have a way to think about it—and to slow down enough to use it.

## A Word of Encouragement

I know this chapter has been about pace and discernment and traffic lights. And that might feel like a lot to hold. You might be thinking, "How do I know which light I'm at?"

Here's what I want you to know: you don't have to have it all figured out. You have to experiment (green-light some low-stakes tries). You have to slow down enough to ask the question (yellow-light the pressure to rush). And you have to refuse the obvious reds (deception, replacement of relationship, formation without presence). The rest is discernment—and that takes time.

So take a breath. Slow down. Use the lights as a map, not a prison. And when you're not sure, err on the side of caution. That's responsible.

---

**Reflection Questions:**

1. Where have you felt pressure to rush with AI? What would it look like to slow down?

2. How might AI help you slow down—by handling tasks that currently force you to rush?

3. Think of one use case in your context. Is it green, yellow, or red? What would need to be true for it to be green?

---

# Chapter 13: What to Refuse and What We're Free to Do

I want to start this chapter by holding two things together: there are things churches and movements must refuse to do with AI. And there are things they're now ethically free—even obligated—to do. Refusal isn't about rejecting AI. It's about protecting what's sacred. Freedom isn't about using AI everywhere. It's about using it where it serves. Both are necessary.

This chapter also names theological integrity as non-negotiable. When you're creating content about faith, theology, and Christian practice, accuracy and depth aren't optional. AI can help—but it can't replace human oversight and formation. That boundary belongs with the other refusals and freedoms.

## What to Refuse

**Fully automated content publishing.** Content generated by AI without human oversight, published without human review or approval. This removes human judgment, eliminates accountability, erodes voice, and undermines credibility. Refuse it.

**AI-generated content without human review.** AI generates; you publish without evaluation. This removes quality control, undermines theological accuracy, and erodes voice. Refuse it.

**Voice replacement.** Using AI to speak or write as if it were you in contexts where relationship or accountability matter—so that people think they're engaging with you when they're not. This deceives and undermines trust. Refuse it.

**Theological content without human verification.** When you're writing about God, Scripture, faith, or Christian practice, AI can help with structure and research—but theological accuracy, biblical grounding, and doctrinal consistency require human oversight. Content that touches theology must be verified by someone formed in that tradition. Refuse publishing theological content without human verification.

**Deceptive practices.** Presenting AI-generated content as fully your own. Hiding AI involvement when it would matter to your audience. Any practice that misrepresents who or what produced the content. Refuse it.

**Formation work without human presence.** Discipleship, pastoral care, mentoring, spiritual direction—these require embodied presence and real relationship. AI can support the content that serves formation; it cannot replace the human presence that formation requires. Refuse formation work without human presence.

## Theological Integrity as Non-Negotiable

If you're a movement leader creating content about faith, theology, and Christian practice, theological integrity isn't optional. It's non-negotiable.

When you're writing about God, Scripture, faith, or Christian practice, accuracy matters. Theological depth matters. Biblical grounding matters. AI can help you communicate more clearly, reach more people, check consistency—but it can't replace your understanding of Scripture, your theological formation, or your commitment to accuracy. AI assistance must align with movemental theology; human oversight and verification are required. When theological integrity is compromised, people can be misled, credibility erodes, and the sacredness of the subject is dishonored. So theological content without human verification is a refusal. And when you do use AI for theological content, you remain responsible for what gets published.

## What We're Free (or Obligated) to Do

**Use AI for amplification.** Using AI to amplify voice, insight, and impact—to make content more discoverable, to reach more people effectively, to multiply impact—while maintaining voice, preserving credibility, and staying transparent. This isn't just allowed; it serves the calling.

**Preserve voice while gaining efficiency.** Using AI to handle logistics (structure, formatting, variation) while preserving voice. Gaining efficiency without losing distinctiveness. Creating more content sustainably while maintaining authenticity. This serves sustainable ministry and allows leaders to focus on what only they can do.

**Build scenius.** Participating in networks of verified humans who vouch for each other, build on each other's ideas, and create collective authority. Credibility through relationships, not just individual metrics. This is something movement leaders are already good at—and now it's how credibility works.

**Be transparent.** Being honest about when and how AI is involved in content creation. Transparency builds trust in an untrustworthy environment. It's foundational, not optional.

**Use specialized agents within boundaries.** Using AI for specific functions—assistants, archivists, translators—within clear boundaries. Not autonomous creators, but tools that amplify what you provide. This is ethically free when you maintain control and stay transparent.

**Create sustainable content creation workflows.** The 70/30 rule (or whatever ratio fits your context): AI handles what it does well; you provide what only you can provide. Sustainable workflows that preserve voice and integrity. This is good stewardship.

## Holding Refusal and Freedom Together

These aren't contradictory. Refusals protect what's sacred. Freedoms serve what's possible. You refuse fully automated publishing so that you can be free to use AI for amplification with human oversight. You refuse theological content without verification so that you can be free to use AI to help with structure and research while you verify the theology. You refuse formation work without presence so that you can be free to use AI to create content that supports the formation that happens in person.

Acknowledge complexity and nuance. Your context will have its own edges. The point is to have a clear set of refusals—and a clear set of freedoms—so that you can discern well.

---

**Reflection Questions:**

1. Which refusals are non-negotiable for you? Which would you add?

2. Which freedoms do you feel ready to step into? Which require more experimentation?

3. How do you hold theological integrity as non-negotiable while still using AI where it helps?

